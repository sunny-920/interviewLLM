import asyncio
import json
import os
import sys
import urllib.parse
from pathlib import Path
from bs4 import BeautifulSoup
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.async_configs import LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from pydantic import BaseModel, Field
from dotenv import load_dotenv

# 1. è®€å–è¨­å®š
env_path = Path(__file__).parent / '.env'
load_dotenv(dotenv_path=env_path)

if not os.getenv("OPENAI_API_KEY"):
    print("âŒ [è‡´å‘½éŒ¯èª¤]ï¼šæ‰¾ä¸åˆ° OPENAI_API_KEY")
    print("   è«‹ç¢ºèªç›®éŒ„ä¸‹æ˜¯å¦æœ‰ .env æª”æ¡ˆï¼Œä¸”å…§å®¹åŒ…å« OPENAI_API_KEY=sk-...")
    sys.exit(1)

# è³‡æ–™çµæ§‹å®šç¾©
class JobAnalysisResult(BaseModel):
    company_name: str = Field(..., description="å…¬å¸åç¨±")
    job_title: str = Field(..., description="è·ä½åç¨±")
    markdown_report: str = Field(..., description="å®Œæ•´çš„è·ç¼ºåˆ†æå ±å‘Šï¼ŒåŒ…å«æŠ€è¡“è¦æ ¼ã€é¢è©¦é¡Œåº«ç­‰ Markdown å…§å®¹")

async def search_duckduckgo_and_get_url(crawler, company, position, platform):
    """
    ç¬¬ä¸€éšæ®µï¼šä½¿ç”¨ DuckDuckGo æœå°‹
    """
    if platform == "104":
        query = f"{company} {position} site:104.com.tw/job/"
        target_domain = "104.com.tw/job/"
    else:
        query = f"{company} {position} site:1111.com.tw/job/"
        target_domain = "1111.com.tw/job/"

    encoded_query = urllib.parse.quote(query)
    ddg_url = f"https://duckduckgo.com/?q={encoded_query}&t=h_&ia=web"
    
    print(f"   â””â”€â”€ ğŸ” æ­£åœ¨ DuckDuckGo æœå°‹: {query}")

    config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        wait_for="body", 
        delay_before_return_html=2.0,
        magic=True
    )
    
    result = await crawler.arun(url=ddg_url, config=config)
    
    if not result.success:
        print(f"   âŒ æœå°‹è«‹æ±‚å¤±æ•—: {result.error_message}")
        return None

    soup = BeautifulSoup(result.html, 'html.parser')
    links = soup.select("a")
    
    for link in links:
        href = link.get('href')
        if href and target_domain in href:
            if "duckduckgo" not in href:
                clean_url = href.split('?')[0]
                print(f"   ğŸ¯ é–å®šç›®æ¨™ç¶²å€: {clean_url}")
                return clean_url

    print("   âš ï¸ è­¦å‘Šï¼šæœå°‹çµæœä¸­æ‰¾ä¸åˆ°ç¬¦åˆçš„è·ç¼ºé€£çµ (å¯èƒ½é—œéµå­—å¤ªæ¨¡ç³Šæˆ–è·ç¼ºå·²ä¸‹æ¶)")
    return None

async def analyze_job_detail(crawler, job_url):
    """
    ç¬¬äºŒéšæ®µï¼šé€²å…¥è©³æƒ…é é€²è¡Œ LLM åˆ†æ
    """
    print(f"   â””â”€â”€ ğŸš€ æ­£åœ¨è¼‰å…¥è·ç¼ºè©³æƒ…é ï¼Œæº–å‚™é€²è¡Œ AI åˆ†æ (é€™å¯èƒ½éœ€è¦ 10-20 ç§’)...")

    llm_config = LLMConfig(
        provider="openai/gpt-4o-mini",
        api_token=os.getenv('OPENAI_API_KEY')
    )

    # === ä½ çš„ PROMPT ä¿æŒåŸæ¨£ ===
    extraction_strategy = LLMExtractionStrategy(
        llm_config=llm_config,
        schema=JobAnalysisResult.model_json_schema(),
        instruction="""
    ä½ æ˜¯ä¸€ä½è³‡æ·±çš„ã€ŒæŠ€è¡“æ‹›å‹Ÿé¡§å•ã€ã€‚
    è«‹æ ¹æ“šæä¾›çš„è·ç¼ºè³‡æ–™ï¼Œæ•´ç†å‡ºçµæ§‹åŒ–çš„é¢è©¦æº–å‚™æª”æ¡ˆã€‚
    
    **é‡è¦æŒ‡ä»¤**ï¼š
    è«‹å°‡æ‰€æœ‰åˆ†æçµæœæ•´ç†æˆä¸€ç¯‡å®Œæ•´çš„ Markdown æ–‡ç« ï¼Œä¸¦å¡«å…¥å›å‚³ JSON çš„ `markdown_report` æ¬„ä½ä¸­ã€‚
    
    Markdown å…§å®¹æ ¼å¼è¦æ±‚å¦‚ä¸‹ï¼š

    # 1. å…¬å¸åŸºæœ¬è­˜åˆ¥è³‡æ–™
    * **å…¬å¸å…¨å**ï¼š
    * **æ‡‰å¾µè·ä½**ï¼š
    * **ç”¢æ¥­é¡åˆ¥**ï¼š
    * **å…¬å¸åœ°é»**ï¼š
    * **ç®¡ç†è²¬ä»»**ï¼š
    * **å‡ºå·®å¤–æ´¾**ï¼š
    * **ä¸Šç­æ™‚æ®µ**ï¼š
    * **ä¼‘å‡åˆ¶åº¦**ï¼š
    * **è–ªæ°´**ï¼š

    # 2. æŠ€è¡“è¦æ ¼åˆ†æ (âš ï¸ é‡é»)
    * **å­¸æ­·è¦æ±‚**ï¼š
    * **èªè¨€æ¢ä»¶**ï¼š
    * **æ ¸å¿ƒç¨‹å¼èªè¨€**ï¼š[ä¾‹å¦‚ Python, Java, C#, JavaScript ç­‰]
    * **å‰ç«¯æŠ€è¡“**ï¼š[ä¾‹å¦‚ React, Vue, HTML/CSS]
    * **å¾Œç«¯èˆ‡è³‡æ–™åº«**ï¼š[ä¾‹å¦‚ Node.js, Spring Boot, MySQL, MongoDB]
    * **é–‹ç™¼å·¥å…·èˆ‡ç’°å¢ƒ**ï¼š[ä¾‹å¦‚ Git, Linux, Docker, AWS]

    # 3. è·ä½è·è²¬
    * **ä¸»è¦å·¥ä½œå…§å®¹**ï¼š
    * **å°ˆæ¡ˆé¡å‹æ¨æ¸¬**ï¼š

    # 4. è»Ÿå¯¦åŠ›èˆ‡æ–‡åŒ–
    * **äººæ ¼ç‰¹è³ª**ï¼š
    * **ç¦åˆ©äº®é»**ï¼š

    # 6. å…¶ä»–é‡è¦è³‡æ–™
    * **ä¸Šé¢çµæ§‹åŒ–å…§å®¹æœªæåŠä½†é‡è¦çš„è³‡æ–™**ï¼š

    # 5. é¢è©¦å®˜æ•™æˆ°é¡Œåº«
    * **å»ºè­°ç™½æ¿é¡Œæ–¹å‘**ï¼š
    * **å»ºè­°ä¸»ç®¡æŠ€è¡“é¡Œ**ï¼š
    * **å»ºè­°HRè¨ªå•å•é¡Œ**ï¼š
        """
    )

    config = CrawlerRunConfig(
        extraction_strategy=extraction_strategy,
        cache_mode=CacheMode.BYPASS,
        wait_for="body", 
        delay_before_return_html=3.0,
        magic=True
    )

    result = await crawler.arun(url=job_url, config=config)

    if result.success:
        try:
            return json.loads(result.extracted_content)
        except:
            print("   âŒ AI å›å‚³è³‡æ–™è§£æéŒ¯èª¤ (JSON Format Error)")
            return None
    else:
        print(f"   âŒ è©³æƒ…é çˆ¬å–å¤±æ•—: {result.error_message}")
        return None

async def main():
    print("\n" + "="*50)
    print("ğŸ¤– AI æ±‚è·é¢è©¦æº–å‚™åŠ©æ‰‹ - å•Ÿå‹•ä¸­")
    print("âš ï¸  æ³¨æ„ï¼šç¨‹å¼åŸ·è¡ŒæœŸé–“æœƒé–‹å•Ÿ Chrome è¦–çª—")
    print("âš ï¸  è«‹å‹¿æ‰‹å‹•é—œé–‰è¦–çª—ï¼Œä»¥å…ç¨‹å¼ä¸­æ–·ï¼")
    print("="*50 + "\n")

    # æª¢æŸ¥è¼¸å…¥æª”æ¡ˆ
    input_file = 'company_input.json'
    if not os.path.exists(input_file):
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° {input_file}")
        print("   è«‹å…ˆåŸ·è¡Œ create_data.py å»ºç«‹è³‡æ–™æª”ã€‚")
        return

    # æª¢æŸ¥ JSON å…§å®¹æ˜¯å¦ç‚ºç©º
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            content = f.read().strip()
            if not content:
                raise ValueError("æª”æ¡ˆå…§å®¹ç‚ºç©º")
            companies = json.loads(content)
    except Exception as e:
        print(f"âŒ éŒ¯èª¤ï¼š{input_file} æ ¼å¼ä¸æ­£ç¢ºæˆ–ç‚ºç©ºã€‚è©³ç´°éŒ¯èª¤: {e}")
        return

    # è¨­å®šç€è¦½å™¨
    browser_cfg = BrowserConfig(
        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
        viewport={"width": 1280, "height": 800},
        headless=False, 
        verbose=True
    )

    results_text = ""

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        for index, item in enumerate(companies, 1):
            print(f"\nğŸ”„ [{index}/{len(companies)}] æ­£åœ¨è™•ç†ï¼š{item['company']} - {item['position']}")
            
            try:
                job_url = await search_duckduckgo_and_get_url(
                    crawler, 
                    item['company'], 
                    item['position'], 
                    item['platform']
                )

                if job_url:
                    extracted_data = await analyze_job_detail(crawler, job_url)
                    
                    if extracted_data:
                        info = extracted_data[0] if isinstance(extracted_data, list) else extracted_data
                        
                        report_content = info.get('markdown_report', 'âš ï¸ åˆ†æå¤±æ•—ï¼šAI æœªå›å‚³æœ‰æ•ˆå ±å‘Š')
                        
                        output_str = f"========================================\n"
                        output_str += f"åˆ†æå°è±¡ï¼š{item['company']} - {item['position']}\n"
                        output_str += f"ä¾†æºç¶²å€ï¼š{job_url}\n"
                        output_str += f"========================================\n\n"
                        output_str += report_content
                        output_str += "\n\n" + ("-" * 50) + "\n\n"
                        
                        print(f"   âœ… åˆ†æå®Œæˆï¼å·²æš«å­˜çµæœã€‚")
                        results_text += output_str
                    else:
                        print(f"   âš ï¸ AI åˆ†æå¤±æ•— (å¯èƒ½å› ç¶²é å…§å®¹éå°‘æˆ–è¢«é˜»æ“‹)")
                        results_text += f"=== {item['company']} ===\n(è©³æƒ…é åˆ†æå¤±æ•—)\n\n"
                else:
                    results_text += f"=== {item['company']} ===\n(DuckDuckGo æœå°‹ç„¡çµæœ)\n\n"

            except Exception as e:
                print(f"   âŒ ç³»çµ±ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")

    # å¯«å…¥æª”æ¡ˆ
    output_file = 'company_profile.txt'
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(results_text)
    
    abs_path = os.path.abspath(output_file)
    print("\n" + "="*50)
    print("ğŸ‰ å…¨éƒ¨ä»»å‹™å®Œæˆï¼")
    print(f"ğŸ“‚ çµæœå·²å„²å­˜è‡³ï¼š{abs_path}")
    print("="*50 + "\n")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ä½¿ç”¨è€…æ‰‹å‹•åœæ­¢ç¨‹å¼ (Ctrl+C)")