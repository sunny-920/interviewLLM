import asyncio
import json
import urllib.parse
import re
import os
from datetime import datetime
from pathlib import Path
from bs4 import BeautifulSoup
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.async_configs import LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from pydantic import BaseModel, Field
import importlib.util

# ================= 1. è¨­å®šèˆ‡ API KEY =================
# å‡è¨­æ­¤æª”æ¡ˆä½æ–¼ project/interview_llm/crawler.py
# æˆ‘å€‘è¦å¾€ä¸Šå…©å±¤æ‰¾åˆ°æ ¹ç›®éŒ„çš„ api_config.py
CURRENT_DIR = Path(__file__).resolve().parent
ROOT_DIR = CURRENT_DIR.parent
API_CONFIG_PATH = ROOT_DIR / "api_config.py"

API_KEY = None
if API_CONFIG_PATH.exists():
    spec = importlib.util.spec_from_file_location("api_config", API_CONFIG_PATH)
    api_config = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(api_config)
    API_KEY = getattr(api_config, "API_KEY", None)

if not API_KEY:
    print("âš ï¸ Warning: API_KEY not found in api_config.py")

# ================= 2. Schema å®šç¾© =================
class JobAnalysisResult(BaseModel):
    company_name: str = Field(..., description="å¾ç¶²é ä¸­æå–çš„çœŸå¯¦æ‹›è˜å…¬å¸åç¨±")
    job_title: str = Field(..., description="å¾ç¶²é ä¸­æå–çš„çœŸå¯¦è·ä½åç¨±")
    markdown_report: str = Field(..., description="åˆ†æå¾Œçš„ markdown å ±å‘Š")

# ================= 3. è¼”åŠ©å‡½å¼ =================
def sanitize_filename(name):
    """ç§»é™¤æª”æ¡ˆåç¨±ä¸­çš„éæ³•å­—å…ƒ"""
    if not name: return "unknown"
    return re.sub(r'[\\/*?:"<>|]', "", name).replace(" ", "_")

async def search_duckduckgo(crawler, company, position):
    """æœå°‹ DuckDuckGo æ‰¾ 104/1111 é€£çµ"""
    print(f"   â””â”€â”€ ğŸ” Search: {company} {position}")
    # å„ªå…ˆæ‰¾ 104ï¼Œä¹Ÿå¯ä»¥åŠ å…¥ 1111
    query = f"{company} {position} (site:104.com.tw/job/ OR site:1111.com.tw/job/)"
    encoded = urllib.parse.quote(query)
    url = f"https://duckduckgo.com/?q={encoded}&t=h_&ia=web"
    
    config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS, 
        wait_for="body", 
        delay_before_return_html=2.0
    )
    result = await crawler.arun(url=url, config=config)
    
    if not result.success: return None
    soup = BeautifulSoup(result.html, "html.parser")
    
    # æ‰¾å°‹æœ€åƒè·ç¼ºçš„é€£çµ
    for link in soup.select("a"):
        href = link.get("href")
        if href and ("104.com.tw/job/" in href or "1111.com.tw/job/" in href) and "duckduckgo" not in href:
            clean_url = href.split("?")[0]
            print(f"   ğŸ¯ Found URL: {clean_url}")
            return clean_url
    return None

async def analyze_page(crawler, url, hint_company, hint_position):
    """é€²å…¥è·ç¼ºé é¢é€²è¡Œ AI åˆ†æ"""
    print(f"   â””â”€â”€ ğŸš€ Analyzing: {url}")
    llm_config = LLMConfig(provider="openai/gpt-4o-mini", api_token=API_KEY)
    
    strategy = LLMExtractionStrategy(
        llm_config=llm_config,
        schema=JobAnalysisResult.model_json_schema(),
        instruction=f"""
    ä½ ç¾åœ¨è¦å¾æ±‚è·ç¶²ç«™çš„HTMLä¸­æå–è³‡è¨Šã€‚
    
    ã€é‡è¦ç›®æ¨™ã€‘ï¼š
    ä½¿ç”¨è€…æ­£åœ¨æœå°‹çš„å…¬å¸æ˜¯ï¼šã€Œ{hint_company}ã€
    ä½¿ç”¨è€…æ­£åœ¨æœå°‹çš„è·ä½æ˜¯ï¼šã€Œ{hint_position}ã€
    
    è«‹åœ¨ç¶²é å…§å®¹ä¸­å°‹æ‰¾ç¬¦åˆä¸Šè¿°ç›®æ¨™çš„ã€ŒçœŸå¯¦å…¬å¸å…¨åã€èˆ‡ã€ŒçœŸå¯¦è·ç¼ºåç¨±ã€ã€‚
    
    ã€ç¦æ­¢äº‹é …ã€‘ï¼š
    1. company_name çµ•å°ä¸èƒ½æ˜¯ "104äººåŠ›éŠ€è¡Œ"ã€"1111äººåŠ›éŠ€è¡Œ" æˆ– "DuckDuckGo"ã€‚
    2. job_title çµ•å°ä¸èƒ½æ˜¯ "æŠ€è¡“æ‹›å‹Ÿé¡§å•" (é‚£æ˜¯ä½ çš„è§’è‰²ï¼Œä¸æ˜¯è·ç¼º)ã€‚
    
    ã€ä»»å‹™ã€‘ï¼š
    è«‹æ ¹æ“šç¶²é å…§å®¹ï¼Œå°‡åˆ†æçµæœæ•´ç†æˆ Markdown å ±å‘Šæ”¾å…¥ `markdown_report` æ¬„ä½ã€‚
    Markdown å…§å®¹éœ€åŒ…å«ï¼š
    # 1. å…¬å¸åŸºæœ¬è­˜åˆ¥è³‡æ–™
    * **å…¬å¸å…¨å**ï¼š
    * **æ‡‰å¾µè·ä½**ï¼š
    * **ç”¢æ¥­é¡åˆ¥**ï¼š
    * **å…¬å¸åœ°é»**ï¼š
    * **ç®¡ç†è²¬ä»»**ï¼š
    * **å‡ºå·®å¤–æ´¾**ï¼š
    * **ä¸Šç­æ™‚æ®µ**ï¼š
    * **ä¼‘å‡åˆ¶åº¦**ï¼š
    * **è–ªæ°´**ï¼š

    # 2. æŠ€è¡“è¦æ ¼åˆ†æ (âš ï¸ é‡é»)
    * **å­¸æ­·è¦æ±‚**ï¼š
    * **èªè¨€æ¢ä»¶**ï¼š
    * **æ ¸å¿ƒç¨‹å¼èªè¨€**ï¼š[ä¾‹å¦‚ Python, Java, C#, JavaScript ç­‰]
    * **å‰ç«¯æŠ€è¡“**ï¼š[ä¾‹å¦‚ React, Vue, HTML/CSS]
    * **å¾Œç«¯èˆ‡è³‡æ–™åº«**ï¼š[ä¾‹å¦‚ Node.js, Spring Boot, MySQL, MongoDB]
    * **é–‹ç™¼å·¥å…·èˆ‡ç’°å¢ƒ**ï¼š[ä¾‹å¦‚ Git, Linux, Docker, AWS]

    # 3. è·ä½è·è²¬
    * **ä¸»è¦å·¥ä½œå…§å®¹**ï¼š
    * **å°ˆæ¡ˆé¡å‹æ¨æ¸¬**ï¼š

    # 4. è»Ÿå¯¦åŠ›èˆ‡æ–‡åŒ–
    * **äººæ ¼ç‰¹è³ª**ï¼š
    * **ç¦åˆ©äº®é»**ï¼š

    # 6. å…¶ä»–é‡è¦è³‡æ–™
    * **ä¸Šé¢çµæ§‹åŒ–å…§å®¹æœªæåŠä½†é‡è¦çš„è³‡æ–™**ï¼š

    # 5. é¢è©¦å®˜æ•™æˆ°é¡Œåº«
    * **å»ºè­°ç™½æ¿é¡Œæ–¹å‘**ï¼š
    * **å»ºè­°ä¸»ç®¡æŠ€è¡“é¡Œ**ï¼š
    * **å»ºè­°HRè¨ªå•å•é¡Œ**ï¼š
        """
    )

    config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS, 
        extraction_strategy=strategy, 
        wait_for="h1",
        delay_before_return_html=3.0
    )
    
    result = await crawler.arun(url=url, config=config)
    
    if result.success:
        try:
            data = json.loads(result.extracted_content)
            item = data[0] if isinstance(data, list) else data
            
            # é˜²å‘†ï¼šè‹¥ AI æŠ“éŒ¯ï¼Œç”¨ user è¼¸å…¥çš„è¦†è“‹
            bad_keywords = ["104", "1111", "äººåŠ›éŠ€è¡Œ"]
            if any(k in item.get("company_name", "") for k in bad_keywords):
                item["company_name"] = hint_company
            
            return item
        except Exception as e:
            print(f"   âŒ JSON Parse Error: {e}")
            return None
    return None

# ================= 4. æ ¸å¿ƒå°å¤–å‡½å¼ (çµ¦ API ç”¨) =================
async def run_crawler(company_name: str, position: str = "è»Ÿé«”å·¥ç¨‹å¸«"):
    """
    é€™æ˜¯ä¸»è¦çš„ Entry Pointã€‚
    å›å‚³ dict: { "summary": ..., "values": ..., "raw_data": ... }
    """
    print(f"ğŸ”„ Crawler started for {company_name}")
    
    browser_cfg = BrowserConfig(headless=True, verbose=False) # Server ä¸Šé€šå¸¸ç”¨ headless=True
    
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        # 1. æœå°‹
        job_url = await search_duckduckgo(crawler, company_name, position)
        if not job_url:
            return {
                "error": "Job URL not found", 
                "company": company_name,
                "summary": "ç„¡æ³•æ‰¾åˆ°è©²å…¬å¸çš„å…¬é–‹è·ç¼ºè³‡è¨Šï¼Œå°‡ä½¿ç”¨é€šç”¨é¢è©¦æ¨¡å¼ã€‚"
            }

        # 2. åˆ†æ
        info = await analyze_page(crawler, job_url, company_name, position)
        if not info:
            return {
                "error": "Analysis failed",
                "company": company_name,
                "summary": "ç„¡æ³•åˆ†æè·ç¼ºé é¢å…§å®¹ã€‚"
            }

        # 3. æ•´ç†å›å‚³è³‡æ–™ (é…åˆ SessionContext æ ¼å¼)
        # æˆ‘å€‘æŠŠ markdown report ç•¶ä½œ context çš„ä¸»è¦ä¾†æº
        report = info.get("markdown_report", "")
        
        # (é¸ç”¨) åŒæ™‚ä¿ç•™åŸæœ¬çš„å­˜æª”é‚è¼¯ï¼Œä½œç‚ºå‚™ä»½
        save_backup_file(info, company_name, position, job_url)

        return {
            "source_url": job_url,
            "company": info.get("company_name", company_name),
            "position": info.get("job_title", position),
            "summary": report,  # é€™è£¡çš„ summary æœƒè¢«é¤µçµ¦ LLM
            "crawled_at": datetime.now().isoformat()
        }

def save_backup_file(info, company, position, url):
    """ä¿ç•™åŸæœ¬çš„å­˜æª”åŠŸèƒ½ä½œç‚º Log"""
    try:
        output_dir = ROOT_DIR / "data" / "crawled_companies"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M")
        safe_name = sanitize_filename(company)
        filename = f"{safe_name}_{timestamp}.txt"
        
        content = (
            f"URL: {url}\n"
            f"Company: {company}\n"
            f"Position: {position}\n"
            f"{'='*30}\n"
            f"{info.get('markdown_report', '')}"
        )
        
        with open(output_dir / filename, "w", encoding="utf-8") as f:
            f.write(content)
        print(f"   ğŸ“‚ Backup saved: {filename}")
    except Exception as e:
        print(f"   âš ï¸ Backup save failed: {e}")